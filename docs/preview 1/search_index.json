[["multiple-regression.html", "7 Multiple regression", " 7 Multiple regression After completing this laboratory exercise, you should be able to: Use R to fit a multiple regression model, and compare the adequacy of several models using inferential and information theo- retic criteria Use R to test hypotheses about the effects of different independent variables on the dependent variable of interest. Use R to evaluate multicollinearity among (supposedly) independent variables and its effects. Use R to do curvilinear (polynomial) regression. "],["set-reg-mul.html", "7.1 R packages and data", " 7.1 R packages and data For this lab you need: R packages: ggplot2 car lmtest simpleboot boot MuMIn data files: Mregdat.csv "],["points-to-keep-in-mind.html", "7.2 Points to keep in mind", " 7.2 Points to keep in mind Multiple regression models are used in cases where there is one dependent variable and several independent, continuous variables. In many biological systems, the variable of interest may be influenced by several different factors, so that accurate description or prediction requires that several independent variables be included in the regression model. Before beginning, be aware that multiple regression takes time to learn well. Beginners should keep in mind several important points: An overall regression model may be statistically significant even if none of the individual regression coefficients in the model are (caused by multicollinearity) A multiple regression model may be ‚Äúnonsignificant‚Äù even though some of the individual coefficients are ‚Äúsignificant‚Äù (caused by overfitting) Unless ‚Äúindependent‚Äù variables are uncorrelated in the sample, different model selection procedures may yield different results. "],["first-look-at-the-data.html", "7.3 First look at the data", " 7.3 First look at the data The file Mregdat.Rdata contains data collected in 30 wetlands in the Ottawa-Cornwall- Kingston area. The data included are the richness (number of species) of: birds (bird , and its log transform logbird), plants (plant, logpl), mammals (mammal, logmam), herptiles (herptile, logherp) total species richness of all four groups combined (totsp, logtot) GPS coordinates of the wetland (lat , long) its area (logarea) the percentage of the wetland covered by water at all times during the year (swamp) the percentage of forested land within 1 km of the wetland (cpfor2) the density (in m/hectare) of hard-surface roads within 1 km of the wetland (thtden). We will focus on herptiles for this exercise, so we better first have a look at how this variable is distributed and correlated to the potential independent variables: mydata &lt;- read.csv(&quot;data/Mregdat.csv&quot;) scatterplotMatrix( ~ logherp + logarea + cpfor2 + thtden + swamp, regLine = TRUE, smooth = TRUE, diagonal = TRUE, data = mydata ) Figure 7.1: Matrice de r√©lation et densit√© pour la richesse sp√©cifique des amphibiens et reptiles "],["multiple-regression-models-from-scratch.html", "7.4 Multiple regression models from scratch", " 7.4 Multiple regression models from scratch We begin the multiple regression exercise by considering a situation with one dependent variable and three (possibly) independent variables. First, we will start from scratch and build a multiple regression model based on what we know from building simple regression models. Next, we will look at automated methods of building multiple regressions models using simultaneous, forward, and backward stepwise procedures. Using the subset of the Mregdat.csv data file, regress logherp on logarea. On the basis of the regression, what do you conclude? model_loga &lt;- lm(logherp ~ logarea, data = mydata) summary(model_loga) ## ## Call: ## lm(formula = logherp ~ logarea, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.38082 -0.09265 0.00763 0.10409 0.46977 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.18503 0.15725 1.177 0.249996 ## logarea 0.24736 0.06536 3.784 0.000818 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1856 on 26 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.3552, Adjusted R-squared: 0.3304 ## F-statistic: 14.32 on 1 and 26 DF, p-value: 0.0008185 par(mfrow = c(2, 2)) plot(model_loga) Figure 7.2: Checking model asusmptions for regression of logherp as a function of logarea It looks like there is a positive relationship between herptile species richness and wetland area: the larger the wetland, the greater the number of species. Note, however, that about 2/3 of the observed variability in species richness among wetlands is not ‚Äúexplained‚Äù by wetland area (R2 = 0.355). Residual analysis shows no major problems with normality, heteroscedasticity or independence of residuals. Rerun the above regression, this time replacing logarea with cpfor2 as the independent variable, such that the expression in the formula field reads: logherp ~ cpfor2 . What do you conclude? model_logcp &lt;- lm(logherp ~ cpfor2, data = mydata) summary(model_logcp) ## ## Call: ## lm(formula = logherp ~ cpfor2, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.49095 -0.10266 0.05881 0.16027 0.25159 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.609197 0.104233 5.845 3.68e-06 *** ## cpfor2 0.002706 0.001658 1.632 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2202 on 26 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.09289, Adjusted R-squared: 0.058 ## F-statistic: 2.662 on 1 and 26 DF, p-value: 0.1148 According to this result, we would accept the null hypothesis, and conclude that there is no relationship between herptile density and the proportion of forest on adjacent lands. But what happens when we enter both variables into the regression simultaneously? Rerun the above regression one more time, this time adding both inde- pendent variables into the model at once, such that logherp ~ logarea + cpfor2 . What do you conclude? model_mcp &lt;- lm(logherp ~ logarea + cpfor2, data = mydata) summary(model_mcp) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40438 -0.11512 0.01774 0.08187 0.36179 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.027058 0.166749 0.162 0.872398 ## logarea 0.247789 0.061603 4.022 0.000468 *** ## cpfor2 0.002724 0.001318 2.067 0.049232 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.175 on 25 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.4493, Adjusted R-squared: 0.4052 ## F-statistic: 10.2 on 2 and 25 DF, p-value: 0.0005774 Now we reject both null hypotheses that the slope of the regression of logherp on logarea is zero and that the slope of the regression of logherp on cpfor2 is zero. Why is cpfor2 a significant predictor of logherp in the combined model when it was not significant in the simple linear model? The answer lies in the fact that it is sometimes necessary to control for one variable in order to detect the effect of another variable. In this case, there is a significant relationship between logherp and logarea that masks the relationship between logherp and cpfor2 . When both variables are entered into the model at once, the effect of logarea is controlled for, making it possible to detect a cpfor2 effect (and vice versa). Run another multiple regression, this time substituting thtden for cpfor2 as an independent variable (logherp ~ logarea + thtden). model_mden &lt;- lm(logherp ~ logarea + thtden, data = mydata) summary(model_mden) ## ## Call: ## lm(formula = logherp ~ logarea + thtden, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31583 -0.12326 0.02095 0.13201 0.31674 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.37634 0.14926 2.521 0.018437 * ## logarea 0.22504 0.05701 3.947 0.000567 *** ## thtden -0.04196 0.01345 -3.118 0.004535 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1606 on 25 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.5358, Adjusted R-squared: 0.4986 ## F-statistic: 14.43 on 2 and 25 DF, p-value: 6.829e-05 In this case we reject the null hypotheses that there are no effects of wetland area ( logarea ) and road density ( thtden ) on herptile richness ( logherp ). Note here that road density has a negative effect on richness, whereas wetland area and forested area ( cpfor2; results from previous regression) both have positive effects on herptile richness. The R2 of this model is even higher than the previous multiple regression model, reflecting a higher correlation between logherp and thtden than between logherp and cpfor2 (if you run a simple regression between logherp and thtden and compare it to the cpfor2 regression you should be able to detect this). Thus far, it appears that herptile richness is related to wetland area ( logarea ), road density ( thtden ), and possibly forest cover on adjacent lands ( cpfor2 ). But, does it necessarily follow that if we build a regression model with all three independent variables, that all three will show significant relationships? No, because we have not yet examined the relationship between Logarea , cpfor2 and thtden . Suppose, for example, two of the variables (say, cpfor2 and thtden ) are perfectly correlated. Then the thtden effect is nothing more than the cpfor2 effect (and vice versa), so that once we include one or the other in the regression model, none of the remaining variability would be explained by the third variable. Fit a regression model with logherp as the dependent variable and logarea , cpfor2 and thtden as the independent variables. What do you conclude? model_mtri &lt;- lm(logherp ~ logarea + cpfor2 + thtden, data = mydata) summary(model_mtri) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2 + thtden, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30729 -0.13779 0.02627 0.11441 0.29582 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.284765 0.191420 1.488 0.149867 ## logarea 0.228490 0.057647 3.964 0.000578 *** ## cpfor2 0.001095 0.001414 0.774 0.446516 ## thtden -0.035794 0.015726 -2.276 0.032055 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1619 on 24 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.5471, Adjusted R-squared: 0.4904 ## F-statistic: 9.662 on 3 and 24 DF, p-value: 0.0002291 Several things to note here: The regression coefficient for cpfor2 has become non-significant: once the variability explained by logarea and thtden is removed, a non-significant part of the remaining variability is explained by cpfor2. The R2 for this model (.547 is only marginally larger than the R2 for the model with only logarea and thtden (.536, which is again consistent with the non-significant coefficient for cpfor2. Note also that although the regression coefficient for thtden has not changed much from that obtained when just thtden and logarea were included in the fitted model (-.036 vs -.042, the standard error for the regression coefficient for thtden has increased slightly, meaning the estimate is less precise. If the correlation between thtden and cpfor2 was greater, the change in precision would also be greater. We can compare the fit of the last two models (i.e., the model with all 3 variables and the model with only logarea and thtden to decide which model is best to include. anova(model_mtri, model_mden) ## Analysis of Variance Table ## ## Model 1: logherp ~ logarea + cpfor2 + thtden ## Model 2: logherp ~ logarea + thtden ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 24 0.62937 ## 2 25 0.64508 -1 -0.015708 0.599 0.4465 Note that this is the identical result we obtained via the t-test of the effect of cpfor2 in the model with all 3 variables above as they are testing the same thing (this should make sense to you). From this analysis, we would conclude that the full model with all three variables included does not offer a significant improvement in fit over the model with only logarea and thtden. This isn‚Äôt surprising given that we already know that we cannot reject the null hypothesis of no effect of cpfor2 in the full model. Overall, we would conclude, on the basis of these analyses, that: Given the three variables thtden , logarea and cpfor2 , the best model is one that includes the first two variables. There is evidence of a negative relationship between herptile richness and the density of roads on adjacent lands. There is evidence that the larger the wetland area, the greater the herptile species richness. Note that by ‚Äúbest‚Äù, I don‚Äôt mean the best possible model, I mean the best one given the three predictor variables we started with. It seems pretty clear that there are other factors controlling richness in wetlands, since even with the ‚Äúbest‚Äù model, almost half of the variability in richness is unexplained. "],["stepwise-multiple-regression-procedures.html", "7.5 Stepwise multiple regression procedures", " 7.5 Stepwise multiple regression procedures There are a number of techniques available for selecting the multiple regression model that best suits your data. When working with only three independent variables it is often sufficient to work through the different combinations of possible variables yourself, until you are satisfied you have fit the best model. This is, essentially, what we did in the first section of this lab. However, the process can become tedious when dealing with numerous independent variables, and you may find an automatic procedure for fitting models to be easier to work with. Stepwise regression in R relies on the Akaike Information Criterion, as a measure of goodness of fit \\[AIC = 2k + 2ln(L))\\] where k is the number of regressors, and L is the maximized value of the likelihood function for the model). This is a statistic that rewards prediction precision while penalizing model complexity. If a new model has an AIC lower than that of the current model, the new model is a better fit to the data. Still working with the Mregdat data, run a stepwise multiple regression on the same set of variables: # Stepwise Regression step_mtri &lt;- step(model_mtri, direction = &quot;both&quot;) ## Start: AIC=-98.27 ## logherp ~ logarea + cpfor2 + thtden ## ## Df Sum of Sq RSS AIC ## - cpfor2 1 0.01571 0.64508 -99.576 ## &lt;none&gt; 0.62937 -98.267 ## - thtden 1 0.13585 0.76522 -94.794 ## - logarea 1 0.41198 1.04135 -86.167 ## ## Step: AIC=-99.58 ## logherp ~ logarea + thtden ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.64508 -99.576 ## + cpfor2 1 0.01571 0.62937 -98.267 ## - thtden 1 0.25092 0.89600 -92.376 ## - logarea 1 0.40204 1.04712 -88.013 step_mtri$anova # display results ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 NA NA 24 0.6293717 -98.26666 ## 2 - cpfor2 1 0.01570813 25 0.6450798 -99.57640 Examining the output, we find: R calculated the AIC for the starting model (here the full model with the 3 independent variables. The AIC for models where terms are deleted. Note here that the only way to reduce the AIC is to drop 2. The AIC for models where terms are added or deleted from the model selected in the first step (i.e.¬†logherp ~ logarea + thtden. Note that none of these models are better. Instead of starting from the full (saturated) model and removing and possibly re-adding terms (i.e.¬†direction = ‚Äúboth‚Äù), one can start from the null model and only add terms: # Forward selection approach model_null &lt;- lm(logherp ~ 1, data = mydata) step_f &lt;- stepAIC( model_null, scope = ~ . + logarea + cpfor2 + thtden, direction = &quot;forward&quot; ) ## Start: AIC=-82.09 ## logherp ~ 1 ## ## Df Sum of Sq RSS AIC ## + logarea 1 0.49352 0.8960 -92.376 ## + thtden 1 0.34241 1.0471 -88.013 ## + cpfor2 1 0.12907 1.2605 -82.820 ## &lt;none&gt; 1.3895 -82.091 ## ## Step: AIC=-92.38 ## logherp ~ logarea ## ## Df Sum of Sq RSS AIC ## + thtden 1 0.25093 0.64508 -99.576 ## + cpfor2 1 0.13078 0.76522 -94.794 ## &lt;none&gt; 0.89600 -92.376 ## ## Step: AIC=-99.58 ## logherp ~ logarea + thtden ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.64508 -99.576 ## + cpfor2 1 0.015708 0.62937 -98.267 step_f$anova # display results ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## logherp ~ 1 ## ## Final Model: ## logherp ~ logarea + thtden ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 27 1.3895281 -82.09073 ## 2 + logarea 1 0.4935233 26 0.8960048 -92.37639 ## 3 + thtden 1 0.2509250 25 0.6450798 -99.57640 You should first notice that the final result is the same as the default stepwise regression and as what we got building the model from scratch. In forward selection, R first fits the least complex model (i.e, with only an intercept), and then adds variables, one by one, according to AIC statistics. Thus, in the above example, the model was first fit with only an intercept. Next, logarea was added, followed by thtden. cpfor2 was not added because it would make AIC increase to above that of the model fit with the first two variables. Generally speaking, when doing multiple regressions, it is good practice to try several different methods (e.g.¬†all regressions, stepwise, and backward elimination, etc.) and see whether you get the same results. If you don‚Äôt, then the ‚Äúbest‚Äù model may not be so obvious, and you will have to think very carefully about the inferences you draw. In this case, regardless of whether we use automatic, or forward/backward stepwise regression, we arrive at the same model. When doing multiple regression, always bear in mind the following: Different procedures may produce different ‚Äúbest‚Äù models, i.e.¬†the ‚Äúbest‚Äù model obtained using forward stepwise regression needn‚Äôt necessarily be the same as that obtained using backward stepwise. It is good practice to try several different methods and see whether you end up with the same result. If you don‚Äôt, it is almost invariably due to multicollinearity among the independent variables. Be wary of stepwise regression. As the authors of SYSTAT, another commonly used statistical package, note: Stepwise regression is probably the most abused computerized statistical technique ever devised. If you think you need automated stepwise regression to solve a particular problem, you probably don‚Äôt. Professional statisticians rarely use automated stepwise regression because it does not necessarily find the ‚Äúbest‚Äù fitting model, the ‚Äúreal‚Äù model, or alternative ‚Äúplausible‚Äù models. Furthermore, the order in which variables enter or leave a stepwise program is usually of no theoretical significance. You are always better off thinking about why a model could generate your data and then testing that model. Remember that just because there is a significant regression of Y on X doesn‚Äôt mean that X causes Y: correlation does not imply causation! "],["detecting-multicollinearity.html", "7.6 Detecting multicollinearity", " 7.6 Detecting multicollinearity Multicollinearity is the presence of correlations among independent variables. In extreme cases (perfect collinearity) it will prevent you from fitting some models. When collinearity is not perfect, it reduces your ability to test for the effect of individual variables, but does not affect the ability of the model to predict. The help file for the HH üì¶package contains this clear passage about one of the indices of multicollinearity, the variance inflation factors: A simple diagnostic of collinearity is the variance inflation factor, VIF one for each regression coefficient (other than the intercept). Since the condition of collinearity involves the predictors but not the response, this measure is a function of the X‚Äôs but not of Y. The VIF for predictor i is \\[1/(1-R_i^2)\\] where Ri2 is the R2 from a regression of predictor i against the remaining predictors. If Ri2 is close to 1, this means that predictor i is well explained by a linear function of the remaining predictors, and, therefore, the presence of predictor i in the model is redundant. Values of VIF exceeding 5 are considered evidence of collinearity: The information carried by a predictor having such a VIF is contained in a subset of the remaining predictors. If, however, all of a model‚Äôs regression coefficients differ significantly from 0 (p-value &lt; .05), a somewhat larger VIF may be tolerable. VIFs indicate by how much the variance of each regression coefficient is increased by the presence of collinearity. There are several vif() functions (I know of at least three in the packages car, HH and DAAG) and I do not know if and how they differ. To quantify multicollinarity, one can simply call the vif() function from the package car: library(car) vif(model_mtri) ## logarea cpfor2 thtden ## 1.022127 1.344455 1.365970 Here there is no evidence that multicollinearity is a problem since all vif are close to 1. "],["polynomial-regression.html", "7.7 Polynomial regression", " 7.7 Polynomial regression In the regression models considered so far, we have assumed that the relationship between the dependent and independent variables is linear. If not, in some cases it can be made linear by transforming one or both variables. On the other hand, for many biological relationships no transformation in the world will help, and we are forced to go with some sort of non-linear regression method. The simplest type of nonlinear regression method is polynomial regression, in which you fit regression models that include independent variables raised to some power greater than one, e.g.¬†X2, X3, etc. Plot the relationship between the residuals of the logherp ~ logarea regression and swamp. # probl√®me avec les donn√©es de manquantes dans logherp mysub &lt;- subset(mydata, !is.na(logherp)) # ajouter les r√©sidus dans les donn√©e mysub$resloga &lt;- residuals(model_loga) ggplot(data = mysub, aes(y = resloga, x = swamp)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Figure 7.3: Relation entre swamp et les r√©sidus de la r√©gression entre logherp et logarea Visual inspection of this graph suggests that there is a strong, but highly nonlinear, relationship between these two variables. Try regressing the residuals of the logherp ~ logarea regression on swamp. What do you conclude? model_resloga &lt;- lm(resloga ~ swamp, mysub) summary(model_resloga) ## ## Call: ## lm(formula = resloga ~ swamp, data = mysub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35088 -0.13819 0.00313 0.10849 0.45802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.084571 0.109265 0.774 0.446 ## swamp -0.001145 0.001403 -0.816 0.422 ## ## Residual standard error: 0.1833 on 26 degrees of freedom ## Multiple R-squared: 0.02498, Adjusted R-squared: -0.01252 ## F-statistic: 0.666 on 1 and 26 DF, p-value: 0.4219 In other words, the fit is terrible, even though you can see from the graph that there is in fact quite a strong relationship between the two - it‚Äôs just that it is a non-linear relationship. (If you look at model assumptions for this model, you will see strong evidence of nonlinearity, as expected.) The pattern might be well described by a quadratic relation. Rerun the above regression but add a second term in the Formula field to represent swamp2 . If you simply add swamp2 in the model R won‚Äôt fit a quadratic effect, you need to use the functionI() which indicates that the formula within should be evaluated before fitting the model. The expression should appear as: \\[ residuals ~ swamp + I(swamp^2)\\]. What do you conclude? What does examination of the residuals from this multiple regression tell you? model_resloga2 &lt;- lm(resloga ~ swamp + I(swamp^2), mysub) summary(model_resloga2) ## ## Call: ## lm(formula = resloga ~ swamp + I(swamp^2), data = mysub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.181185 -0.085350 0.007377 0.067327 0.242455 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.804e-01 1.569e-01 -4.975 3.97e-05 *** ## swamp 3.398e-02 5.767e-03 5.892 3.79e-06 *** ## I(swamp^2) -2.852e-04 4.624e-05 -6.166 1.90e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1177 on 25 degrees of freedom ## Multiple R-squared: 0.6132, Adjusted R-squared: 0.5823 ## F-statistic: 19.82 on 2 and 25 DF, p-value: 6.972e-06 par(mfrow = c(2, 2)) plot(model_resloga2) It is clear that once the effects of area are controlled for, a considerable amount of the remaining variability in herptile richness is explained by swamp , in a nonlinear fashion. If you examine model assumptions, you will see that compared to the linear model, the fit is much better. Based on the results from the above analyses, how would you modify the regression model arrived at above? What, in your view, is the ‚Äúbest‚Äù overall model? Why? How would you rank the various factors in terms of their effects on herptile species richness? In light of these results, we might want to try and fit a model which includes logarea, thtden, cpfor2, swamp and swamp^2^ : model_poly1 &lt;- lm( logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2), data = mydata ) summary(model_poly1) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2), ## data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.201797 -0.056170 -0.002072 0.051814 0.205626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.203e-01 1.813e-01 -1.766 0.0912 . ## logarea 2.202e-01 3.893e-02 5.656 1.09e-05 *** ## cpfor2 -7.864e-04 9.955e-04 -0.790 0.4380 ## thtden -2.929e-02 1.048e-02 -2.795 0.0106 * ## swamp 3.113e-02 5.898e-03 5.277 2.70e-05 *** ## I(swamp^2) -2.618e-04 4.727e-05 -5.538 1.45e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1072 on 22 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.8181, Adjusted R-squared: 0.7767 ## F-statistic: 19.78 on 5 and 22 DF, p-value: 1.774e-07 Note that on the basis of this analysis, we could potentially drop cpfor2 and refit using the remaining variables: model_poly2 &lt;- lm( logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata ) summary(model_poly2) ## ## Call: ## lm(formula = logherp ~ logarea + thtden + swamp + I(swamp^2), ## data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19621 -0.05444 -0.01202 0.07116 0.21295 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.461e-01 1.769e-01 -1.957 0.0626 . ## logarea 2.232e-01 3.842e-02 5.810 6.40e-06 *** ## thtden -2.570e-02 9.364e-03 -2.744 0.0116 * ## swamp 2.956e-02 5.510e-03 5.365 1.89e-05 *** ## I(swamp^2) -2.491e-04 4.409e-05 -5.649 9.46e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1063 on 23 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.8129, Adjusted R-squared: 0.7804 ## F-statistic: 24.98 on 4 and 23 DF, p-value: 4.405e-08 How about multicollinearity in this model? vif(model_poly2) ## logarea thtden swamp I(swamp^2) ## 1.053193 1.123491 45.845845 45.656453 VIF for the two swamp terms are much higher than the standard threshold of 5. However, this is expected for polynomial terms, and not really a concern given that both terms are highly significant in the model. The high VIF means that these two coefficients are not estimated precisely, but using both in the model still allows to make a good prediction (i.e.¬†account for the response to swamp). "],["checking-assumptions-of-a-multiple-regression-model.html", "7.8 Checking assumptions of a multiple regression model", " 7.8 Checking assumptions of a multiple regression model All the model selection techniques or the manual model crafting assumes that the standard assumptions (independence, normality, homoscedasticity, linearity) are met. Given that a large number of models can be fitted, it may seem that testing the assumptions at each step would be an herculean task. However, it is generally sufficient to examine the residuals of the full (saturated) model and of the final model. Terms not contributing significantly to the fit do not affect residuals much, and therefore, the residuals to the full model, or the residuals to the final model, are generally sufficient. Let‚Äôs have a look at the diagnostic plots for the final model. Here we use the check_model() function from the performance üì¶. library(performance) check_model(model_poly2) Figure 7.4: Conditions d‚Äôapplication du mod√®le model_poly2 Alternatively it can be done with the classic method par(mfrow = c(2, 2)) plot(model_poly2) Figure 7.5: Conditions d‚Äôapplication du mod√®le model_poly2 Everything looks about right here. For the skeptic, let‚Äôs run the formal tests. shapiro.test(residuals(model_poly2)) ## ## Shapiro-Wilk normality test ## ## data: residuals(model_poly2) ## W = 0.9837, p-value = 0.9278 The residuals do not deviate from normality. Good. library(lmtest) bptest(model_poly2) ## ## studentized Breusch-Pagan test ## ## data: model_poly2 ## BP = 3.8415, df = 4, p-value = 0.4279 No deviation from homoscedasticity either. Good. dwtest(model_poly2) ## ## Durbin-Watson test ## ## data: model_poly2 ## DW = 1.725, p-value = 0.2095 ## alternative hypothesis: true autocorrelation is greater than 0 No serial correlation in the residuals, so no evidence of non-independence. resettest(model_poly2, type = &quot;regressor&quot;, data = mydata) ## ## RESET test ## ## data: model_poly2 ## RESET = 0.9823, df1 = 8, df2 = 15, p-value = 0.4859 And no significant deviation from linearity. So it seems that all is fine. "],["visualizing-effect-size.html", "7.9 Visualizing effect size", " 7.9 Visualizing effect size Les coefficients de la r√©gression multiple peuvent mesurer la taille d‚Äôeffet, quoiqu‚Äôil puisse √™tre n√©cessaire de les standardiser pour qu‚Äôils ne soient pas influenc√©s par les unit√©s de mesure. Mais un graphique est souvent plus informatif. Dans ce contexte, les graphiques des r√©sidus partiels (appel√©s components+residual plots dans R) sont particuli√®rement utiles. Ces graphique illustrent comment la variable d√©pendante, corrig√©e pour l‚Äôeffet des autres variables dans le mod√®le, varie avec chacune des variables ind√©pendantes du mod√®le. Voyons voir: # Evaluate visually linearity and effect size # component + residual plot crPlots(model_poly2) Figure 7.6: Graphiques de r√©sidus partiels du mod√®le model_poly2 Notez que l‚Äô√©chelle de l‚Äôaxe des y varie sur chaque graphique. Pour thtden, la variable d√©pendante (log10(richesse des herptiles)) varie d‚Äôenviron 0.4 unit√©s entre la valeur minimum et maximum de thtden. Pour logarea, la variation est d‚Äôenviron 0.6 unit√© log. Pour swamp, l‚Äôinterpr√©tation est plus compliqu√©e parce qu‚Äôil y a deux termes qui quantifient son effet, et que ces termes ont des signes oppos√©s (positif pour swamp et n√©gatif pour swamp^2) ce qui donne une relation curvilin√©aire de type parabole. Le graphique ne permet pas de bien visualiser cela. Ceci dit, ces graphique n‚Äôindiquent pas vraiment de violation de lin√©arit√©. Pour illustrer ce qui serait visible sur ces graphiques si il y avait une d√©viation de lin√©arit√©, enlevons le terme du second degr√© pour swamp, puis on va refaire ces graphiques et effectuer le test RESET. model_nopoly &lt;- lm( logherp ~ logarea + thtden + swamp, data = mydata ) crPlots(model_nopoly) Figure 7.7: Graphiques de r√©sidus partiels du mod√®le model_nopoly La relation non-lin√©aire avec swamp devient √©vidente. Et le test RESET d√©tecte bien cette non-lin√©arit√©: resettest(model_nopoly, type = &quot;regressor&quot;) ## ## RESET test ## ## data: model_nopoly ## RESET = 6.7588, df1 = 6, df2 = 18, p-value = 0.0007066 "],["tester-la-pr√©sence-dinteractions.html", "7.10 Tester la pr√©sence d‚Äôinteractions", " 7.10 Tester la pr√©sence d‚Äôinteractions Lorsqu‚Äôil y a plusieurs variables ind√©pendantes, vous devriez toujours garder √† l‚Äôesprit la possibilit√© d‚Äôinteractions. Dans la majorit√© des situations de r√©gression multiple cela n‚Äôest pas √©vident parce que l‚Äôaddition de termes d‚Äôinteraction augmente la multicolin√©arit√© des termes du mod√®le, et parce qu‚Äôil n‚Äôy a souvent pas assez d‚Äôobservations pour √©prouver toutes les interactions ou que les observations ne sont pas suffisamment balanc√©es pour faire des tests puissants pour les interactions. Retournons √† notre mod√®le ‚Äúfinal‚Äù et voyons ce qui se passe si on essaie d‚Äôajuster un mod√®le satur√© avec toutes les interactions: fullmodel_withinteractions &lt;- lm( logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2), data = mydata ) summary(fullmodel_withinteractions) ## ## Call: ## lm(formula = logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2), ## data = mydata) ## ## Residuals: ## ALL 28 residuals are 0: no residual degrees of freedom! ## ## Coefficients: (4 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.948e+03 NaN NaN NaN ## logarea 3.293e+03 NaN NaN NaN ## cpfor2 7.080e+01 NaN NaN NaN ## thtden 9.223e+02 NaN NaN NaN ## swamp 1.176e+02 NaN NaN NaN ## I(swamp^2) -3.517e-01 NaN NaN NaN ## logarea:cpfor2 -3.771e+01 NaN NaN NaN ## logarea:thtden -4.781e+02 NaN NaN NaN ## cpfor2:thtden -1.115e+01 NaN NaN NaN ## logarea:swamp -7.876e+01 NaN NaN NaN ## cpfor2:swamp -1.401e+00 NaN NaN NaN ## thtden:swamp -1.920e+01 NaN NaN NaN ## logarea:I(swamp^2) 5.105e-01 NaN NaN NaN ## cpfor2:I(swamp^2) 3.825e-03 NaN NaN NaN ## thtden:I(swamp^2) 7.826e-02 NaN NaN NaN ## swamp:I(swamp^2) -2.455e-03 NaN NaN NaN ## logarea:cpfor2:thtden 5.359e+00 NaN NaN NaN ## logarea:cpfor2:swamp 8.743e-01 NaN NaN NaN ## logarea:thtden:swamp 1.080e+01 NaN NaN NaN ## cpfor2:thtden:swamp 2.620e-01 NaN NaN NaN ## logarea:cpfor2:I(swamp^2) -5.065e-03 NaN NaN NaN ## logarea:thtden:I(swamp^2) -6.125e-02 NaN NaN NaN ## cpfor2:thtden:I(swamp^2) -1.551e-03 NaN NaN NaN ## logarea:swamp:I(swamp^2) -4.640e-04 NaN NaN NaN ## cpfor2:swamp:I(swamp^2) 3.352e-05 NaN NaN NaN ## thtden:swamp:I(swamp^2) 2.439e-04 NaN NaN NaN ## logarea:cpfor2:thtden:swamp -1.235e-01 NaN NaN NaN ## logarea:cpfor2:thtden:I(swamp^2) 7.166e-04 NaN NaN NaN ## logarea:cpfor2:swamp:I(swamp^2) NA NA NA NA ## logarea:thtden:swamp:I(swamp^2) NA NA NA NA ## cpfor2:thtden:swamp:I(swamp^2) NA NA NA NA ## logarea:cpfor2:thtden:swamp:I(swamp^2) NA NA NA NA ## ## Residual standard error: NaN on 0 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 1, Adjusted R-squared: NaN ## F-statistic: NaN on 27 and 0 DF, p-value: NA Notez les coefficients manquants aux derni√®res lignes: on ne peut inclure les 32 termes si on a seulement 28 observations. Il manque des observations, le R carr√© est 1, et le mod√®le ‚Äúpr√©dit‚Äù parfaitement les donn√©es. Si on essaie une m√©thode automatique pour identifier le ‚Äúmeilleur‚Äù mod√®le dans ce g√¢chis, R refuse: step(fullmodel_withinteractions) ## Error in step(fullmodel_withinteractions): AIC is -infinity for this model, so &#39;step&#39; cannot proceed Bon, est-ce qu‚Äôon oublie tout √ßa et qu‚Äôon accepte le mod√®le final sans ce soucier des interactions? Non, pas encore. Il y a un compromis possible: comparer notre mod√®le ‚Äúfinal‚Äù √† un mod√®le qui contient au moins un sous-ensemble des interactions, par exemple toutes les interactions du second degr√©, pour √©prouver si l‚Äôaddition de ces interactions am√©liore beaucoup l‚Äôajustement du mod√®le. full_model_2ndinteractions &lt;- lm( logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + thtden:swamp, data = mydata ) summary(full_model_2ndinteractions) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + ## logarea:cpfor2 + logarea:thtden + logarea:swamp + cpfor2:thtden + ## cpfor2:swamp + thtden:swamp, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.216880 -0.036534 0.003506 0.042990 0.175490 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.339e-01 6.325e-01 0.686 0.502581 ## logarea -1.254e-01 2.684e-01 -0.467 0.646654 ## cpfor2 -9.344e-03 7.205e-03 -1.297 0.213032 ## thtden -1.833e-01 9.035e-02 -2.028 0.059504 . ## swamp 3.569e-02 7.861e-03 4.540 0.000334 *** ## I(swamp^2) -3.090e-04 7.109e-05 -4.347 0.000500 *** ## logarea:cpfor2 2.582e-03 2.577e-03 1.002 0.331132 ## logarea:thtden 7.017e-02 3.359e-02 2.089 0.053036 . ## logarea:swamp -5.290e-04 2.249e-03 -0.235 0.816981 ## cpfor2:thtden -2.095e-04 6.120e-04 -0.342 0.736544 ## cpfor2:swamp 4.651e-05 5.431e-05 0.856 0.404390 ## thtden:swamp 2.248e-04 4.764e-04 0.472 0.643336 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.108 on 16 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.8658, Adjusted R-squared: 0.7735 ## F-statistic: 9.382 on 11 and 16 DF, p-value: 4.829e-05 Ce mod√®le s‚Äôajuste un peu mieux aux donn√©es que les mod√®le ‚Äúfinal‚Äù (il explique 86.6% de la variance de logherp, compar√© √† 81.2% pour le mod√®le ‚Äúfinal‚Äù sans interactions), mais il compte deux fois plus de param√®tres. De plus, si vous examinez les coefficients, il se passe d‚Äô√©tranges choses: le signe pour logare a chang√© par exemple. C‚Äôest un des sympt√¥mes de la multicolin√©arit√©. Allons voir les facteurs d‚Äôinflation de la variance: vif(full_model_2ndinteractions) ## logarea cpfor2 thtden swamp I(swamp^2) ## 49.86060 78.49622 101.42437 90.47389 115.08457 ## logarea:cpfor2 logarea:thtden logarea:swamp cpfor2:thtden cpfor2:swamp ## 66.97792 71.69894 67.27034 14.66814 29.41422 ## thtden:swamp ## 20.04410 Aie! tous les VIF sont plus grands que 5, pas seulement les termes incluant swamp. Cette forte multicolin√©arit√© emp√™che de quantifier avec pr√©cision l‚Äôeffet de ces interactions. De plus, ce mod√®le avec interactions n‚Äôest pas plus informatif que le mod√®le ‚Äúfinal‚Äù puisque son AIC est plus √©lev√© (souvenez-vous qu‚Äôon privil√©gie le mod√®le avec la valeur d‚ÄôAIC la plus basse): AIC(model_poly1) ## [1] -38.3433 AIC(full_model_2ndinteractions) ## [1] -34.86123 On peut √©galement utiliser la fonction anova() pour comparer l‚Äôajustement des deux mod√®les et v√©rifier si l‚Äôaddition des termes d‚Äôint√©ration am√©liore significativement l‚Äôajustement: anova(model_poly1, full_model_2ndinteractions) ## Analysis of Variance Table ## ## Model 1: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) ## Model 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + ## logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + ## thtden:swamp ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 22 0.25282 ## 2 16 0.18651 6 0.066314 0.9481 0.489 Ici, l‚Äôaddition des termes d‚Äôinteraction ne r√©duit pas significativement la variabilit√© r√©siduelle du mod√®le ‚Äúcomplet‚Äù. Qu‚Äôen est-il de la si on compare le mod√®le avec interaction et notre mod√®le ‚Äúfinal‚Äù? anova(model_poly2, full_model_2ndinteractions) ## Analysis of Variance Table ## ## Model 1: logherp ~ logarea + thtden + swamp + I(swamp^2) ## Model 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + ## logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + ## thtden:swamp ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 23 0.25999 ## 2 16 0.18651 7 0.073486 0.9006 0.5294 Le test indique que ces deux mod√®les ont des variances r√©siduelles comparables, et donc que l‚Äôaddition des termes d‚Äôinteraction et de cpfor2 au mod√®le final n‚Äôapporte pas grand chose. "],["recherche-du-meilleur-mod√®le-fond√©e-sur-la-th√©orie-de-linformation.html", "7.11 Recherche du meilleur mod√®le fond√©e sur la th√©orie de l‚Äôinformation", " 7.11 Recherche du meilleur mod√®le fond√©e sur la th√©orie de l‚Äôinformation Une des principales critiques des m√©thodes pas-√†-pas (stepwise) est que les valeurs de p ne sont pas strictement interpr√©tables √† cause du grand nombre de tests qui sont implicites dans le processus. C‚Äôest le probl√®me des comparaisons ou tests multiples: en construisant un mod√®le lin√©aire (comme une r√©gression multiple) √† partir d‚Äôun grand nombre de variables et de leurs interactions, il y a tellement de combinaisons possibles qu‚Äôun ajustement de Bonferroni rendrait les tests trop conservateurs. Une alternative, d√©fendue par Burnham et Anderson (2002, Model selection and multimodel inference: a practical information-theoretic approach. 2nd ed), est d‚Äôutiliser l‚ÄôAIC (ou mieux encore AICc qui est plus appropri√© quand le nombre d‚Äôobservations est inf√©rieur √† 40 fois le nombre de variables ind√©pendantes) pour ordonner les mod√®les et identifier un sousensemble de mod√®les qui sont les meilleurs. On peut ensuite calculer les moyennes des coefficients pond√©r√©es par la probabilit√© que chacun des mod√®les soit le meilleur pour obtenir des coefficients qui sont plus robustes et moins sensibles √† la multicolin√©arit√©. L‚Äôapproche de comparaison par AIC a d‚Äôabord √©t√© d√©velopp√© pour comparer un ensemble de mod√®le pr√©alablement d√©fini bas√© sur les connaissance du syt√®me et les hypoth√®ses biologiques. Cependant, certains ont d√©velopp√© une approche plut√¥t brutale et sans scrupule de faire tous les mod√®les possibles et de les comparer par AIC. Cette approche a √©t√© suivie dans le package MuMIn. Les comparaisons de mod√®le par AICdoivent √™tre faites en utilisant exactement le m√™me jeu de donn√©es pour chaque mod√®le. Il faut donc s‚Äôarrurer d‚Äôenlever les donn√©es manquantes et de sp√©cifier dans la fonction lm de ne pas marcher s‚Äôil y a des donn√©es manquantes. Je ne supporte pas l‚Äôapproche stepwise ni l‚Äôapproche par AIC. Je d√©teste l‚Äôapproche par la fonction dredge() qui selon moi va √† l‚Äôencontre de la philosophie des AIC et de la parsimonie. Je soutiens de d√©velooper un mod√®le bas√© sur des hypoth√®ses biologiques et de reporter ce mod√®le avec tous les effets significatifs ou non. # refaire le mod√®le en s&#39;assurant qu&#39;il n&#39;y a pas de &quot;NA&quot; # et en sp√©cificant na.action full_model_2ndinteractions &lt;- update( full_model_2ndinteractions, . ~ ., data = mysub, na.action = &quot;na.fail&quot; ) library(MuMIn) dd &lt;- dredge(full_model_2ndinteractions) ## Fixed term is &quot;(Intercept)&quot; L‚Äôobjet dd contient tous les mod√®les possibles (i.e.¬†ceux qui ont toutes les combinaisons possibles) en utilisant les termes du mod√®le full_model_2ndinteractions ajust√© pr√©c√©demment. On peut ensuite extraire de l‚Äôobjet dd le sous-ensemble de mod√®les qui ont un AICc semblable au meilleur mod√®le (Burnham et Anderson sugg√®rent que les mod√®les qui d√©vient par plus de 7 unit√©s d‚ÄôAICc du meilleur mod√®le ont peu de support empirique). # get models within 2 units of AICc from the best model top_models_1 &lt;- get.models(dd, subset = delta &lt; 2) avgmodel1 &lt;- model.avg(top_models_1) # compute average parameters summary(avgmodel1) # display averaged model ## ## Call: ## model.avg(object = top_models_1) ## ## Component model call: ## lm(formula = logherp ~ &lt;2 unique rhs&gt;, data = mysub, na.action = na.fail) ## ## Component models: ## df logLik AICc delta weight ## 12345 7 27.78 -35.95 0.00 0.55 ## 1234 6 25.78 -35.56 0.39 0.45 ## ## Term codes: ## I(swamp^2) logarea swamp thtden logarea:thtden ## 1 2 3 4 5 ## ## Model-averaged coefficients: ## (full average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) -2.145e-01 2.308e-01 2.406e-01 0.891 0.373 ## logarea 1.356e-01 1.089e-01 1.119e-01 1.213 0.225 ## swamp 3.180e-02 5.971e-03 6.273e-03 5.070 4e-07 *** ## I(swamp^2) -2.669e-04 4.770e-05 5.011e-05 5.326 1e-07 *** ## thtden -6.985e-02 5.233e-02 5.361e-02 1.303 0.193 ## logarea:thtden 2.131e-02 2.487e-02 2.545e-02 0.837 0.403 ## ## (conditional average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) -2.145e-01 2.308e-01 2.406e-01 0.891 0.3727 ## logarea 1.356e-01 1.089e-01 1.119e-01 1.213 0.2253 ## swamp 3.180e-02 5.971e-03 6.273e-03 5.070 4e-07 *** ## I(swamp^2) -2.669e-04 4.770e-05 5.011e-05 5.326 1e-07 *** ## thtden -6.985e-02 5.233e-02 5.361e-02 1.303 0.1927 ## logarea:thtden 3.882e-02 2.114e-02 2.237e-02 1.735 0.0827 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 confint(avgmodel1) # display CI for averaged coefficients ## 2.5 % 97.5 % ## (Intercept) -0.6860022996 0.257064603 ## logarea -0.0836067896 0.354883299 ## swamp 0.0195105703 0.044099316 ## I(swamp^2) -0.0003650809 -0.000168656 ## thtden -0.1749296690 0.035236794 ## logarea:thtden -0.0050266778 0.082666701 La liste des mod√®les qui sont √† 4 unit√©s ou moins de l‚ÄôAICc du meilleur mod√®le. Les variables dans chaque mod√®le sont cod√©es et on retrouve la cl√© en dessous du tableau. Pour chaque mod√®le, en plus de l‚ÄôAICc, le poids Akaike est calcul√©. C‚Äôest un estim√© de la probabilit√© que ce mod√®le est le meilleur. Ici on voit que le premier mod√®le (le meilleur) a seulement 34% des chance d‚Äô√™tre vraiment le meilleur. √Ä partir de ce sous-ensemble de mod√®les, la moyenne pond√©r√©e des coefficients (en utilisant les poids Akaike) est calcul√©e, avec in IC √† 95%. Notez que les termes absents d‚Äôun mod√®le sont consid√©r√©s avoir un coefficient de 0 pour ce terme. "],["bootstrap-et-r√©gression-multiple.html", "7.12 Bootstrap et r√©gression multiple", " 7.12 Bootstrap et r√©gression multiple Quand les donn√©es ne rencontrent pas les conditions d‚Äôapplication de normalit√© et d‚Äôhomosc√©dasticit√© et que les transformations n‚Äôarrivent pas √† corriger ces violations, le bootstrap peut √™tre utilis√© pour calculer des intervalles de confiance pour les coefficients. Si la distribution des coefficients bootstrapp√©s est sym√©trique et approximativement normale, on peut utiliser les percentiles empiriques pour calculer les limites de confiance. Le code qui suit, utilisant le package simpleboot, a √©t√© con√ßu pour √™tre facilement modifiable et calcule les limites des IC √† partir des percentiles empiriques. ############################################################ ####### # Bootstrap analysis the simple way with library simpleboot # Define model to be bootstrapped and the data source used mymodel &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata) # Set the number of bootstrap iterations nboot &lt;- 1000 library(simpleboot) # R is the number of bootstrap iterations # Setting rows to FALSE indicates resampling of residuals mysimpleboot &lt;- lm.boot(mymodel, R = nboot, rows = FALSE) # Extract bootstrap coefficients myresults &lt;- sapply(mysimpleboot$boot.list, function(x) x$coef) # Transpose matrix so that lines are bootstrap iterations # and columns are coefficients tmyresults &lt;- t(myresults) Vous pouvez ensuite faire des graphiques pour voir les r√©sultats. Lorsque vous tournerez ce code, il y aura une pause pour vous permettre d‚Äôexaminer la distribution pour chaque coefficient du mod√®le sur des graphiques: # Plot histograms of bootstrapped coefficients ncoefs &lt;- length(data.frame(tmyresults)) par(mfrow = c(1, 2), mai = c(0.5, 0.5, 0.5, 0.5), ask = TRUE) for (i in 1:ncoefs) { lab &lt;- colnames(tmyresults)[i] x &lt;- tmyresults[, i] plot(density(x), main = lab, xlab = &quot;&quot;) abline(v = mymodel$coef[i], col = &quot;red&quot;) abline(v = quantile(x, c(0.025, 0.975))) hist(x, main = lab, xlab = &quot;&quot;) abline(v = quantile(x, c(0.025, 0.975))) abline(v = mymodel$coef[i], col = &quot;red&quot;) } Figure 7.8: Distribution des estim√© par bootstrap pour logarea Le graphique de droite illustre la densit√© liss√©e (kernel density) et celui de gauche est l‚Äôhistogramme des estim√©s bootstrap du coefficient. La ligne rouge sur le graphique indique la valeur du coefficient ordinaire (pas bootstrap) et les deux lignes verticales noires marquent les limites de l‚Äôintervalle de confiance √† 95%. Ici l‚ÄôIC ne contient pas 0, et donc on peut conclure que l‚Äôeffet de logarea sur logherp est significativement positif. Les limites pr√©cises peuvent √™tre obtenues par: # Display empirical bootstrap quantiles (not corrected for bias) p &lt;- c(0.005, 0.01, 0.025, 0.05, 0.95, 0.975, 0.99, 0.995) apply(tmyresults, 2, quantile, p) ## (Intercept) logarea thtden swamp I(swamp^2) ## 0.5% -0.75190854 0.1437474 -0.047894948 0.01610660 -0.0003549979 ## 1% -0.71503313 0.1499690 -0.045785709 0.01710535 -0.0003498040 ## 2.5% -0.65877271 0.1590264 -0.042492649 0.01961127 -0.0003335924 ## 5% -0.61683811 0.1680194 -0.040369219 0.02117658 -0.0003180897 ## 95% -0.07537645 0.2783590 -0.011475445 0.03801454 -0.0001830251 ## 97.5% -0.02203447 0.2893995 -0.009071042 0.04022615 -0.0001688640 ## 99% 0.01373337 0.3015650 -0.005676100 0.04227593 -0.0001470395 ## 99.5% 0.03300897 0.3125414 -0.004550531 0.04324638 -0.0001397980 Ces intervalles de confiances ne sont pas fiables si la distribution des estim√©s bootstrap n‚Äôest pas Gaussienne. Dans ce cas, il vaut mieux calculer des coefficients non-biais√©s (bias-corrected accelerated confidence limits, BCa): ################################################ # Bootstrap analysis in multiple regression with BCa confidence intervals # Preferable when parameter distribution is far from normal # Bootstrap 95% BCa CI for regression coefficients library(boot) # function to obtain regression coefficients for each iteration bs &lt;- function(formula, data, indices) { d &lt;- data[indices, ] # allows boot to select sample fit &lt;- lm(formula, data = d) return(coef(fit)) } # bootstrapping with 1000 replications results &lt;- boot( data = mydata, statistic = bs, R = 1000, formula = logherp ~ logarea + thtden + swamp + I(swamp^2) ) # view results Pour obtenir les r√©sultats, le code suivant va produire le graphique standard pour chaque coefficient, et les estim√©s BCa pour l‚Äôintervalle de confiance plot(results, index = 1) # intercept plot(results, index = 2) # logarea plot(results, index = 3) # thtden plot(results, index = 4) # swamp plot(results, index = 5) # swamp2 # get 95% confidence intervals boot.ci(results, type = &quot;bca&quot;, index = 1) boot.ci(results, type = &quot;bca&quot;, index = 2) boot.ci(results, type = &quot;bca&quot;, index = 3) boot.ci(results, type = &quot;bca&quot;, index = 4) boot.ci(results, type = &quot;bca&quot;, index = 5) Pour logarea, cela donne: ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;bca&quot;, index = 2) ## ## Intervals : ## Level BCa ## 95% ( 0.1118, 0.3194 ) ## Calculations and Intervals on Original Scale Notez que l‚Äôintervalle BCa va de 0.12 √† 0.32, alors que l‚Äôintervalle standard √©tait de 0.16 √† 0.29. L‚Äôintervalle BCa est ici plus grand du c√¥t√© inf√©rieur et plus petit du c√¥t√© sup√©rieur comme il se doit compte tenu de la distribution non-Gaussienne et asym√©trique des estim√©s bootstrap. "],["perm_reg_mult.html", "7.13 Test de permutation", " 7.13 Test de permutation Les tests de permutations sont plus rarement effectu√©s en r√©gression multiple que le bootstrap. Voici un fragment de code pour le faire tout de m√™me. ############################################################ ########## # Permutation in multiple regression # # using lmperm library library(lmPerm) # Fit desired model on the desired dataframe my_model &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata ) my_model_prob &lt;- lmp( logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata, perm = &quot;Prob&quot; ) summary(my_model) summary(my_model_prob) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
